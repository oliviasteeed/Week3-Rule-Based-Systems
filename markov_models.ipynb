{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliviasteeed/Week3-Rule-Based-Systems/blob/main/markov_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4deL4w__jV1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAT-ComputationalCreativity-Spring2025/Week3-Rule-Based-Systems/blob/main/markov_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wffLgyef_jV4"
      },
      "source": [
        "# Markov Models Text Generation\n",
        "\n",
        "This notebook introduces Markov chains for text generation. We'll build a simple\n",
        "text generator that learns patterns from input text and generates new text with\n",
        "similar statistical properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmjpKhG1_jV5"
      },
      "outputs": [],
      "source": [
        "# First, let's import our required libraries\n",
        "from collections import defaultdict\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_7kG9TI_jV7"
      },
      "source": [
        "## Building the Markov Chain\n",
        "\n",
        "A Markov chain represents sequences of states where the probability of each state\n",
        "depends only on the previous state(s). In our case, each state will be a sequence\n",
        "of words, and we'll predict the next word based on this sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Oj_71h3_jV7"
      },
      "outputs": [],
      "source": [
        "def build_markov_chain(text, order=2):\n",
        "    \"\"\"\n",
        "    Build a Markov chain from input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to learn from\n",
        "        order (int): Number of words to use as state (context)\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping from state tuples to lists of possible next words\n",
        "    \"\"\"\n",
        "    chain = defaultdict(list)\n",
        "    words = text.split()\n",
        "\n",
        "    for i in range(len(words) - order):\n",
        "        # Create state tuple from current words\n",
        "        state = tuple(words[i:i+order])\n",
        "        # Get the next word\n",
        "        next_word = words[i+order]\n",
        "        # Add to chain\n",
        "        chain[state].append(next_word)\n",
        "\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3zv1rI4_jV8"
      },
      "source": [
        "## Generating Text\n",
        "\n",
        "Now we'll use our Markov chain to generate new text. We'll randomly select from\n",
        "the possible next words at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_QUm4Gm_jV9"
      },
      "outputs": [],
      "source": [
        "def generate_text(chain, num_words=30):\n",
        "    \"\"\"\n",
        "    Generate new text using the Markov chain.\n",
        "\n",
        "    Args:\n",
        "        chain (dict): Markov chain mapping states to possible next words\n",
        "        order (int): Length of state tuples\n",
        "        num_words (int): Number of words to generate\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text\n",
        "    \"\"\"\n",
        "    # Start with a random state from the chain\n",
        "    words = list(random.choice(list(chain.keys())))\n",
        "    order = len(list(chain.keys())[0])\n",
        "\n",
        "    for _ in range(num_words - order):\n",
        "        state = tuple(words[-order:])\n",
        "        if state in chain:\n",
        "            next_word = random.choice(chain[state])\n",
        "            words.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddPNIyb_jV9"
      },
      "source": [
        "## Part 3: Basic Example\n",
        "\n",
        "Let's try our text generator with a simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeM7Oldt_jV-",
        "outputId": "af0f12e4-60d2-4fc9-c86b-83ebc86a81eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick -> ['brown']\n",
            "quick brown -> ['fox', 'dog', 'dog']\n",
            "brown fox -> ['jumps']\n",
            "fox jumps -> ['over']\n",
            "jumps over -> ['the', 'the']\n",
            "over the -> ['lazy', 'lazy']\n",
            "the lazy -> ['dog.', 'fox.']\n",
            "lazy dog. -> ['A']\n",
            "dog. A -> ['quick']\n",
            "A quick -> ['brown']\n",
            "brown dog -> ['jumps', 'watches.']\n",
            "dog jumps -> ['over']\n",
            "lazy fox. -> ['The']\n",
            "fox. The -> ['lazy']\n",
            "The lazy -> ['fox']\n",
            "lazy fox -> ['sleeps']\n",
            "fox sleeps -> ['while']\n",
            "sleeps while -> ['the']\n",
            "while the -> ['quick']\n",
            "the quick -> ['brown']\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "A quick brown dog jumps over the lazy fox.\n",
        "The lazy fox sleeps while the quick brown dog watches.\n",
        "\"\"\"\n",
        "\n",
        "# Build the chain\n",
        "chain = build_markov_chain(sample_text)\n",
        "\n",
        "# Examine the chain\n",
        "for state, words in chain.items():\n",
        "    print(f\"{' '.join(state)} -> {words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L0BT-B5_jV-",
        "outputId": "74c130b1-d137-4cfa-d356-e6d133d0d5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "over the lazy dog. A quick brown dog jumps over the lazy fox. The lazy fox sleeps while the quick brown dog watches.\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "print(\"Generated text:\")\n",
        "print(generate_text(chain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHYD-Pn8_jV_"
      },
      "source": [
        "## Student Tasks\n",
        "\n",
        "1. Basic Implementation:\n",
        "   - Try changing the order parameter in build_markov_chain\n",
        "   - What happens with order=1 vs order=3?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl7wHMyv_jV_",
        "outputId": "e921ca7d-6f3f-4fb9-b1d7-0476d4d2bf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Order 1:\n",
            "jumps over the lazy dog. A quick brown dog jumps over the lazy fox. The quick brown dog watches.\n",
            "\n",
            "Order 3:\n",
            "The lazy fox sleeps while the quick brown dog watches.\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Experiment with different orders\n",
        "print(\"\\nOrder 1:\")\n",
        "chain1 = build_markov_chain(sample_text, order=1)\n",
        "print(generate_text(chain1))\n",
        "\n",
        "print(\"\\nOrder 3:\")\n",
        "chain3 = build_markov_chain(sample_text, order=8)\n",
        "print(generate_text(chain3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8nWiSAC_jV_"
      },
      "source": [
        "2. Use Your Own Text:\n",
        "   Below, try using a different text source. You could use:\n",
        "   - Song lyrics\n",
        "   - Book excerpts\n",
        "   - Movie quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj3Pf34Z_jV_"
      },
      "outputs": [],
      "source": [
        "# Task 2: Add your own text here\n",
        "your_text = \"\"\"\n",
        "Rule-based Systems\n",
        "- rules have been around forever - formal (rituals, laws), informal (logic, language) have long history (the modus ponens Aristotle)\n",
        "- computing - loops and conditions (rules) allow for interactivity\n",
        "- expert systems: nuclear plants, planes - directed by humans and expert systems (not AI, all rule-based)\n",
        "Types\n",
        "- Generative grammars and Chomsky hierarchy (natural language is formalized using rule-based systems)\n",
        "- Formal Logic systems and logic programming: reasoning rules, theorem solver, argumentation systems, case-based reasoning systems\n",
        "- Expert Systems: expert knowledge in particular domain encoded into formal framework allowing deduction\n",
        "- Transition-based Networks: finite state automata/stochastic systems\n",
        "##### Generative Grammars: describe the syntax of language\n",
        "- syntax of natural language is grammar based on dictionary\n",
        "- also true of formal (computing) languages\n",
        "- **Human Language**: syntax (how to put together), semantics (what it means), pragmatics (how do we use it - turn taking, rituals, when to use what language)\n",
        "\t- syntax is combinatorial - various categories of linguistic syntactic units: nouns, verbs, adjectives, articles - assembled into larger units of sentences following certain rules\n",
        "\t- with a grammar we can test if given sentence follows rules, generate new sentences by using rules\n",
        "- Rewrite Rule: rule in which left side re\n",
        "COPY FROM SLIDES\n",
        "- Grammar: vocabulary, set of terminal symbols, set of rules, an initial symbol\n",
        "- if you use rules until terminal you can always generate a new sentence\n",
        "- syntactic correctness does not mean semantic correctness - this is big issue of grammars\n",
        "##### The Chomsky Hierarchy: typology for types of generative grammars, correspond exactly to mathematical functions and automata\n",
        "- **Type 0 Grammar**: no restrictions on either side od production rules. can be many terminal or non-terminal symbols on either side\n",
        "\t- Respective formal language: recursively enumerable language or partially decidable language\n",
        "\t- Automaton: non-deterministic Turing machine\n",
        "\t- Generative Capacity: very high\n",
        "\t- Complexity: undecidable (complexity theory - proving if we can or cannot execute algorithm on computer)\n",
        "\t- Expressive: the most, too complex to be used in practice\n",
        "- **Type 1 Grammar**: number of symbols on right must be larger than or equal to number of symbols on left\n",
        "\t- Formal language: context sensitive - what comes before is taken into account\n",
        "\t- Automaton: linear-bounded automaton\n",
        "\t- Generative capacity: high\n",
        "\t- Complexity: exponential\n",
        "\t- undecidable: given an expression the computer can always decide if in language or not in finite time\n",
        "- **Type 2 Grammar**: context-free grammar, left side production rules consists of one single non-terminal symbol\n",
        "\t- Formal Languages: context-free, programming languages\n",
        "- **Type 3 Grammars**: context-free grammar\n",
        "\t- Restrictions: left hand side is single non-terminal, right-hand side is terminal followed by one non-terminal at most\n",
        "\t- Formal Language: regular languages\n",
        "\t- Automaton: deterministic finite automaton or non-deterministic finite automation\n",
        "- Determinism and Indeterminism\n",
        "\t- choice between several rules - can pick randomly, heuristic, usually would associate probabilities to each rule - sum of all probabilities need to be 1 (100% chance you pick a rule)\n",
        "\n",
        "Grammars in Generative Art\n",
        "- before deep learning every program used grammars - even google translate learned grammars by inference for input text\n",
        "- used for describing, analyzing, generating music - dominated by European structured classical music but has some about all\n",
        "Music - grammar of chords and combinations, structure piece with the grammar - no matter what you generate it gives jazz sounding 8-chord sequence\n",
        "- Impro-Visor: generates MIDI bars based on input sequence - still needs someone to play it to sound good\n",
        "Literature/Poetry: generative grammars used as means for procedural text generation\n",
        "- Linear test: flows from beginning to end\n",
        "- Non-linear structure: interactive, can follow different paths and has to make choices - most common is hypertext\n",
        "- SCIgen: machine learning papers\n",
        "- Postmodern writing\n",
        "Pro\n",
        "- can encode hierarchy, recurrence\n",
        "- light\n",
        "- explicit knowledge - can read and understand, will never hallucinate\n",
        "- light - can generate fast\n",
        "Con\n",
        "- rigid, limited\n",
        "- encode syntax not semantics\n",
        "\n",
        "Transition Networks\n",
        "- finite-state automata in which nodes are states and edges are transitions\n",
        "- each automata stands for non-terminal symbol\n",
        "- each transition produces a non-terminal or terminal symbol\n",
        "- set of automata is same as a grammar\n",
        "\n",
        "L-systems: type of grammar created to model growth of plants, algae and trees, functions apply to all values at once\n",
        "- no distinction between terminal and non-terminal symbols - all symbols represent an action\n",
        "- triplet (v, w, P) where v is alphabet, w is initiator, P is finite set of production rules - predecessor has to be only one alphabet element, successor can be 0+ elements\n",
        "- can make fractals, plant shapes are made using grammars\n",
        "- Parametric L-systems: attach parameters like time, size\n",
        "- Context-Sensitive L-system: different production rules to apply in different contexts, even more powerful\n",
        "- Stochastic L-Systems: can be many productions for single symbol, can pick random or add probabilities to options\n",
        "L-system in Use:\n",
        "- patterns/symmetric drawing, interactive art generative plants, organic-looking visuals (find L-systems for actual plants and then model with graphics), architecture, visual art, music (start with initiator, variation building off this)\n",
        "\n",
        "Markov Models (read generative model): model sequences of discrete events: words, notes - given previous will generate next\n",
        "- X random variable representing note at time t\n",
        "- P(X) probability distribution of events represented by random variable X at time step\n",
        "- from this can get probability of each note\n",
        "- modeling conditional probability distribution - want to know what note to play based on context previous notes\n",
        "- LLM and GPT is giant Markov chain - given what generated in the past or the prompt, what should I generate next?\n",
        "- Markov Assumption: the future only depends on the present and a limited part of the past - say d past elements\n",
        "- Finite State Automata: each state is a node and transitions are edges weighted by transition probabilities\n",
        "- Viable Order Markov Model (VMM/VOMM): predict conditional probability distribution at any level of depth\n",
        "Markov Chain in Use: first generative music was using Markov Chain\n",
        "- Continuator: VMM - gives probability for next notes based on what played\n",
        "- Beatback: interactive drum - continues to play in your style so you can play other drums\n",
        "- Harmonic Progression Generator - load in artist, gives you progression that sounds like them\n",
        "- in text\n",
        "Pro\n",
        "- intuitive, easy to understand\n",
        "- cheap\n",
        "Con\n",
        "- randomness in output\n",
        "- lack of overall structure - short attention span, will meander, if you have too short of corpus it will copy the piece instead of generating new\n",
        "- higher order risk to have limited branching so will make a lot of \"quotes\" from corpus\n",
        "- limited to one-dimensional symbolic sequences\n",
        "- limited to style imitation, parody, pastiche\n",
        "Hidden Markov Model (HMM): actual state of system becomes hidden and only accessible through emission probabilities\n",
        "- used to learn coupling between two streams - in rational problem solving (weather and sensor data), in computational creativity for music\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"My Text:\")\n",
        "chain4 = build_markov_chain(your_text, order=7)\n",
        "print(generate_text(chain4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfoXI0lDsij",
        "outputId": "ec551cb0-a056-4b60-c5ac-b8ac0a6ffe5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My Text:\n",
            "- given what generated in the past or the prompt, what should I generate next? - Markov Assumption: the future only depends on the present and a limited part of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1WOxTJ5_jV_"
      },
      "source": [
        "3. Advanced Implementation:\n",
        "   Add temperature-based sampling to control randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzBzKJEk_jWA",
        "outputId": "eed469a2-c5ac-4a46-fcab-022d4d938786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Low temperature (more predictable):\n",
            "giant Markov chain - given what generated in the past or the prompt, what should I generate next? - Markov Assumption: the future only depends on the present and a\n",
            "\n",
            "High temperature (more random):\n",
            "have too short of corpus it will copy the piece instead of generating new - higher order risk to have limited branching so will make a lot of \"quotes\" from\n"
          ]
        }
      ],
      "source": [
        "def generate_text_with_temperature(chain, temperature=1.0, num_words=30):\n",
        "    \"\"\"\n",
        "    Generate text with temperature-based sampling.\n",
        "    Lower temperature = more conservative/predictable\n",
        "    Higher temperature = more random/creative\n",
        "\n",
        "    Args:\n",
        "        chain (dict): Markov chain\n",
        "        temperature (float): Controls randomness (0.1 to 2.0 recommended)\n",
        "        order (int): Length of state tuples\n",
        "        num_words (int): Number of words to generate\n",
        "    \"\"\"\n",
        "    words = list(random.choice(list(chain.keys())))\n",
        "    order = len(list(chain.keys())[0])\n",
        "\n",
        "    for _ in range(num_words - order):\n",
        "        state = tuple(words[-order:])\n",
        "        if state in chain:\n",
        "            # Count frequencies of next words\n",
        "            next_words = chain[state]\n",
        "            word_counts = defaultdict(int)\n",
        "            for word in next_words:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "            # Apply temperature\n",
        "            weights = [count ** (1.0 / temperature) for count in word_counts.values()]\n",
        "            total = sum(weights)\n",
        "            weights = [w/total for w in weights]\n",
        "\n",
        "            # Choose next word based on weighted probabilities\n",
        "            next_word = random.choices(list(word_counts.keys()), weights=weights)[0]\n",
        "            words.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Try different temperatures\n",
        "print(\"\\nLow temperature (more predictable):\")\n",
        "print(generate_text_with_temperature(chain4, temperature=0.1))\n",
        "\n",
        "print(\"\\nHigh temperature (more random):\")\n",
        "print(generate_text_with_temperature(chain4, temperature=4.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJirFHNl_jWA"
      },
      "source": [
        "## Challenge Tasks:\n",
        "\n",
        "1. Implement a function to analyze the Markov chain:\n",
        "   - Count the number of unique states\n",
        "   - Find the most common transitions\n",
        "   - Calculate the average number of possible next words for each state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMTL3DP-_jWA",
        "outputId": "71bdad7e-a5f3-49f2-fb26-30119c79daa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chain Analysis:\n",
            "Number of unique states: 20\n",
            "Average transitions per state: 1.30\n",
            "\n",
            "Most common transitions:\n",
            "The quick -> brown (count: 1)\n",
            "quick brown -> dog (count: 2)\n",
            "brown fox -> jumps (count: 1)\n",
            "fox jumps -> over (count: 1)\n",
            "jumps over -> the (count: 2)\n"
          ]
        }
      ],
      "source": [
        "def analyze_chain(chain):\n",
        "    \"\"\"\n",
        "    Analyze properties of the Markov chain.\n",
        "\n",
        "    Args:\n",
        "        chain (dict): Markov chain to analyze\n",
        "    \"\"\"\n",
        "    num_states = len(chain)\n",
        "    total_transitions = sum(len(next_words) for next_words in chain.values())\n",
        "    avg_transitions = total_transitions / num_states if num_states > 0 else 0\n",
        "\n",
        "    # Find most common next word for each state\n",
        "    most_common = {}\n",
        "    for state, next_words in chain.items():\n",
        "        word_counts = defaultdict(int)\n",
        "        for word in next_words:\n",
        "            word_counts[word] += 1\n",
        "        most_common[state] = max(word_counts.items(), key=lambda x: x[1])\n",
        "\n",
        "    print(f\"Number of unique states: {num_states}\")\n",
        "    print(f\"Average transitions per state: {avg_transitions:.2f}\")\n",
        "    print(\"\\nMost common transitions:\")\n",
        "    for state, (word, count) in list(most_common.items())[:5]:  # Show top 5\n",
        "        print(f\"{' '.join(state)} -> {word} (count: {count})\")\n",
        "\n",
        "# Analyze our chain\n",
        "print(\"\\nChain Analysis:\")\n",
        "analyze_chain(chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BlUaDs__jWA"
      },
      "source": [
        "## Further Exploration:\n",
        "\n",
        "Other ideas to try:\n",
        "1. Modify the code to preserve punctuation\n",
        "2. Add start-of-sentence and end-of-sentence tokens\n",
        "3. Implement bi-directional generation\n",
        "4. Create a chain that works with characters instead of words\n",
        "5. Add input validation and error handling"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iat460",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}